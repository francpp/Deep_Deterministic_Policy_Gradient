{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac032619-0db7-45e3-941e-2847c2648aa7",
   "metadata": {},
   "source": [
    "# DDPG PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab7139-73cb-447a-a74d-a57266a2df1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import gym and define pendulum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d135534-5fa8-45dc-ba4d-6a16de9f6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104252cc-7c45-40c8-9ce4-0491b2261bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#env = gym.make('Pendulum-v1', g=9.81, render_mode = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07697ca8-6849-459e-baa9-2f0d30088e33",
   "metadata": {},
   "source": [
    "Check API-conformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c0451-5435-4c7d-9b30-b649bbeec6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e5c47-7c70-43df-8674-912373b53cc8",
   "metadata": {},
   "source": [
    "Import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b7f9c-5f4e-4ac1-b67a-07c6db0e0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503181b-a59e-4c22-9e8e-37600c011295",
   "metadata": {},
   "source": [
    "## 3) Heuristic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb838f9-6a3c-4c82-8749-f236921503b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Wrap environment in a NormalizedEnv class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966ae2b-ed43-481b-8058-bdf9ed86db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)\n",
    "#env = gym.make('Pendulum-v1', g=9.81, render_mode = 'human')\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883213a-65b7-45df-b3be-bebcb5b455eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NormalizedEnv(env)\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28085c-6490-4b0b-82a2-5eb686020613",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f65200-e673-4d8c-8573-31fdb4b0cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0700bfc-bb4d-4106-9d64-328901544e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.zeros((10,200))\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    \n",
    "    trunc = False\n",
    "    cur_reward = []\n",
    "    \n",
    "    while not trunc:   \n",
    "        action = random_agent.compute_action(state) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        state = next_state\n",
    "        cur_reward.append(reward)\n",
    "        \n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "    \n",
    "    rewards[i] = cur_reward\n",
    "\n",
    "rand_rewards = rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faa4b7-9765-42b4-a235-ff9f6340461f",
   "metadata": {},
   "source": [
    "##### Heuristic pendulum agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3a7ee-afed-40ed-9b3a-b63fc897c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_agent = HeuristicPendulumAgent(env, 0.8)\n",
    "rewards = np.zeros((10,200))\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    trunc = False\n",
    "    cur_reward = []\n",
    "\n",
    "    while not trunc:\n",
    "        action = heur_agent.compute_action(state) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        state = next_state\n",
    "        cur_reward.append(reward)\n",
    "\n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "            \n",
    "    rewards[i] = cur_reward\n",
    "\n",
    "heur_rewards = rewards\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(200), np.mean(rand_rewards, axis = 0))\n",
    "plt.plot(np.arange(200), np.mean(heur_rewards, axis = 0))\n",
    "plt.legend(['Random','Heuristic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99283af-3e3d-48e5-b053-c62d4a014882",
   "metadata": {},
   "source": [
    "Effect of fixed torque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b97b23-9993-4908-803c-ea227bd1100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torques = np.linspace(-1,1,30)\n",
    "avg_rewards = []\n",
    "\n",
    "for t in torques:\n",
    "    heur_agent = HeuristicPendulumAgent(env,t)\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "\n",
    "        trunc = False\n",
    "        cur_reward = 0\n",
    "\n",
    "        while not trunc:\n",
    "            action = heur_agent.compute_action(state) \n",
    "            (next_state, reward, term, trunc, info) = env.step(action)\n",
    "            state = next_state\n",
    "            cur_reward += reward\n",
    "\n",
    "            if term or trunc:\n",
    "                observation, info = env.reset()\n",
    "\n",
    "        rewards.append(cur_reward)\n",
    "    avg_rewards.append(np.mean(rewards))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torques, avg_rewards)\n",
    "plt.xlabel('Fixed torque')\n",
    "plt.ylabel('Avg. total reward (over 10 runs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b527f-b284-4dd9-b4b6-639a95ea9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06883194-3076-4df2-9eed-49e43198c4a5",
   "metadata": {},
   "source": [
    "## 4) Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059d34f-8a8a-4854-b894-f2d6382ac9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583916-f509-4050-bf71-d62989cd9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, agent, optimizer, criterion, gamma, Buffer, epoch, device):\n",
    "    model.eval()\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    Trunc = False\n",
    "    loss_total = []\n",
    "    while not Trunc:\n",
    "        with torch.no_grad():\n",
    "            action = agent.compute_action(state) \n",
    "        (next_state, reward, term, Trunc, info) = env.step(action)\n",
    "        Buffer.store_transition((state, action, reward, next_state, Trunc))\n",
    "        state = next_state\n",
    "        #print(Buffer.buffer[-1])\n",
    "        if (len(Buffer.buffer) >= 128):\n",
    "            model.train()\n",
    "            b = Buffer.batch_buffer(128)\n",
    "            data = np.zeros((128,4))\n",
    "            reward = np.zeros(128)\n",
    "            next_data = np.zeros((128,4))\n",
    "            trunc = np.zeros(128)\n",
    "            for i,transition in enumerate(b):  \n",
    "                data[i] = np.concatenate((transition[0], transition[1]))\n",
    "                reward[i] = transition[2]\n",
    "                trunc[i] = transition[4]\n",
    "                with torch.no_grad():           \n",
    "                    next_data[i] = np.concatenate((transition[3], agent.compute_action(transition[3])))\n",
    "\n",
    "            data = torch.tensor(data, dtype=torch.float32).to(device)   \n",
    "            reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "            next_data = torch.tensor(next_data, dtype=torch.float32).to(device)\n",
    "            trunc = torch.tensor(trunc, dtype = torch.bool)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)  # (state, action) -> Q(s, a)\n",
    "            #print(output)\n",
    "            with torch.no_grad():\n",
    "                target = (reward + gamma*model(next_data).flatten()).reshape(128,1)\n",
    "            #print(target)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total.append(loss.item())\n",
    "            \n",
    "    \n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d45f57-95c7-485a-89a1-63184cd82016",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_agent = HeuristicPendulumAgent(env,0.8)\n",
    "max_size = 1e4\n",
    "\n",
    "device = \"cpu\"\n",
    "model = QNetwork().to(device)\n",
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 2\n",
    "history = []\n",
    "Buffer = ReplayBuffer(max_size = max_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, heur_agent, optimizer, criterion, gamma, Buffer, epoch, device)\n",
    "    if epoch%10 == 0:\n",
    "        print(loss[-1])\n",
    "    history.extend(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008a495-dad9-4d0f-9590-309613089e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb09ae-d722-4e84-9a3a-223e6df959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history)), history)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('MSE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bf827-3e0f-4532-9e64-4b85152a175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(alpha, alpha_speed, torque):\n",
    "    return -(alpha**2 + 0.1*alpha_speed**2 + 0.001*torque**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd29bd-4ea2-4015-ac18-18c48622e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "logs = [(0,0),(0,1),(2,1),(-2,1),(-2,-1)]\n",
    "\n",
    "for l in logs:\n",
    "    speed = l[0]\n",
    "    torque = l[1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    rad = np.linspace(0, 5, 100)\n",
    "    azm = np.linspace(-np.pi, np.pi, 100)\n",
    "    r, th = np.meshgrid(rad, azm)\n",
    "    \n",
    "    # Q-Network\n",
    "    speed_array = speed*np.ones(r.shape)\n",
    "    torque_array = torque*np.ones(r.shape)\n",
    "    data = np.stack([np.cos(th), np.sin(th), speed_array, torque_array], axis = 2)\n",
    "    data = torch.tensor(data, dtype = torch.float32)\n",
    "    z = model(data)\n",
    "    z = z.detach().numpy().squeeze()\n",
    "\n",
    "    ax1 = fig.add_subplot(121, projection=\"polar\")\n",
    "    ax1.grid(False)\n",
    "    pc = ax1.pcolormesh(th, r, z)\n",
    "    ax1.plot(azm, r, color='k', ls='none') \n",
    "    ax1.set_theta_zero_location('N')\n",
    "    fig.colorbar(pc)\n",
    "\n",
    "    # Original heuristic\n",
    "    rad = np.linspace(0, 5, 100)\n",
    "    azm = np.linspace(-np.pi, np.pi, 100)\n",
    "    r, th = np.meshgrid(rad, azm)\n",
    "    z_heur = compute_reward(th, speed, torque*np.sign(np.cos(th))*np.sign(speed))\n",
    "    \n",
    "    ax2= fig.add_subplot(122,projection=\"polar\")\n",
    "    ax2.grid(False)\n",
    "    pc2 = ax2.pcolormesh(th, r, z_heur)\n",
    "    ax2.plot(azm, r, color='k', ls='none') \n",
    "    ax2.set_theta_zero_location('N')\n",
    "    fig.colorbar(pc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6905cb-cf0a-4665-ab00-b5476f0cb3c2",
   "metadata": {},
   "source": [
    "## 5) Minimal Implementation of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df199106-b4ac-4910-98af-49bd11b22ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_policy(state, action, model_Q):\n",
    "    #with torch.no_grad():\n",
    "    #model_Q.eval()\n",
    "    #action.detach().numpy()  \n",
    "    data = torch.cat((state, action), axis = 1)\n",
    "    #data = torch.tensor(data, dtype=torch.float32).to(device)   \n",
    "    loss_policy = torch.mean(-model_Q(data))\n",
    "    #loss_policy.requires_grad = True\n",
    "    return loss_policy\n",
    "\n",
    "\n",
    "def train_epoch_policy(model_policy, model_Q, agent, optimizer, criterion, gamma, Buffer, epoch, device):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    Trunc = False\n",
    "    loss_total = []\n",
    "    while not Trunc:\n",
    "        model_policy.eval()\n",
    "        with torch.no_grad():\n",
    "            action = agent.compute_action(state)\n",
    "        (next_state, reward, term, Trunc, info) = env.step(action)\n",
    "        Buffer.store_transition((state, action, reward, next_state, Trunc))\n",
    "        state = next_state\n",
    "    \n",
    "        if len(Buffer.buffer) >= 128:\n",
    "            model_Q.eval()\n",
    "            model_policy.train()\n",
    "            b = Buffer.batch_buffer(128)\n",
    "            data = np.zeros((128,3))\n",
    "            reward = np.zeros(128)\n",
    "            next_data = np.zeros((128,3))\n",
    "            trunc = np.zeros(128)\n",
    "            for i,transition in enumerate(b):        \n",
    "                data[i] = transition[0]\n",
    "                reward[i] = transition[2]\n",
    "                trunc[i] = transition[4]\n",
    "                next_data[i] = transition[3]\n",
    "\n",
    "            data = torch.tensor(data, dtype=torch.float32).to(device)   \n",
    "            reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "            next_data = torch.tensor(next_data, dtype=torch.float32).to(device)\n",
    "            trunc = torch.tensor(trunc, dtype = torch.bool)\n",
    "            optimizer.zero_grad()\n",
    "            output = model_policy(data)  # state -> action\n",
    "            \n",
    "            loss = compute_loss_policy(data, output, model_Q)\n",
    "            # loss = criterion(loss_p, torch.zeros([128,1], requires_grad = True))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total.append(loss.item())\n",
    "\n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91c0de-758a-421c-9b87-d03fec3273b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "# Define DDPG agent\n",
    "noise_std = 0.3\n",
    "ddpg_agent = DDPGAgent(noise_std, device)\n",
    "\n",
    " \n",
    "# model_Q = QNetwork().to(device)\n",
    "# model_policy = PolicyNetwork().to(device)\n",
    "\n",
    "max_size = 1e5\n",
    "Buffer = ReplayBuffer(max_size = max_size)\n",
    "\n",
    "\n",
    "# Training parameters, ecc...\n",
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "optimizer_Q = torch.optim.Adam(ddpg_agent.Q_network.parameters(), lr = lr)\n",
    "optimizer_policy = torch.optim.Adam(ddpg_agent.policy_network.parameters(), lr = lr)\n",
    "criterion_Q = torch.nn.MSELoss()\n",
    "criterion_policy = torch.nn.L1Loss()\n",
    "num_epochs = 5 # metti 1000\n",
    "\n",
    "history_Q = []\n",
    "history_policy = []\n",
    "\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    action = ddpg_agent.compute_action(state, deterministic = False) \n",
    "    (next_state, reward, term, trunc, info) = env.step(action)\n",
    "    Buffer.store_transition((state, action, reward, next_state, trunc))\n",
    "    state = next_state\n",
    "\n",
    "    if term or trunc:\n",
    "        observation, info = env.reset()\n",
    "        # Save reward of episode\n",
    "    \n",
    "    loss_Q = train_epoch(ddpg_agent.Q_network, ddpg_agent, optimizer_Q, criterion_Q, gamma, Buffer, epoch, device)\n",
    "    loss_policy = train_epoch_policy(ddpg_agent.policy_network, ddpg_agent.Q_network, ddpg_agent, optimizer_policy, criterion_policy, gamma, Buffer, epoch, device)\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('lq = ', loss_Q[-1])\n",
    "        print('lp = ', loss_policy[-1])\n",
    "    history_Q.extend(loss_Q)\n",
    "    history_policy.extend(loss_policy)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6415b-3a22-48ee-801d-cffebd33031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_Q)\n",
    "plt.plot(history_policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75977cd0-f62f-49d1-a170-f663683970ad",
   "metadata": {},
   "source": [
    "Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eaad6d-0439-4c90-90de-ba268051c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('Pendulum-v1', g=9.81, render_mode = 'human')\n",
    "\n",
    "# rewards = np.zeros((5,200))\n",
    "\n",
    "# for i in range(5):\n",
    "#     state = env.reset()\n",
    "#     state = state[0]\n",
    "    \n",
    "#     trunc = False\n",
    "#     cur_reward = []\n",
    "    \n",
    "#     while not trunc:   \n",
    "#         action = ddpg_agent.compute_action(model_policy, state, deterministic = True) \n",
    "#         (next_state, reward, term, trunc, info) = env.step(action)\n",
    "#         state = next_state\n",
    "#         cur_reward.append(reward)\n",
    "        \n",
    "#         if term or trunc:\n",
    "#             observation, info = env.reset()\n",
    "    \n",
    "#     rewards[i] = cur_reward\n",
    "\n",
    "# rand_rewards = rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17efb1b-8d94-4caf-b4ab-4be3cab9535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('Pendulum-v1', g=9.81)\n",
    "rewards = np.zeros((100,200))\n",
    "\n",
    "#Buffer = ReplayBuffer(max_size = 400)\n",
    "\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    trunc = False\n",
    "    cur_reward = []\n",
    "\n",
    "    while not trunc:\n",
    "        action = ddpg_agent.compute_action(state, deterministic = True) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        state = next_state\n",
    "        cur_reward.append(reward)\n",
    "\n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "            \n",
    "    rewards[i] = cur_reward\n",
    "\n",
    "ddpg_rewards = rewards\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(200), np.mean(rand_rewards,axis = 0))\n",
    "plt.plot(np.arange(200), np.mean(heur_rewards, axis = 0))\n",
    "plt.plot(np.arange(200), np.mean(ddpg_rewards, axis = 0))\n",
    "\n",
    "plt.legend(['Random','Heuristic','DDPG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6260e5-3229-4ce1-a7be-b04de6b58af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = [(0,0),(0,1),(2,1),(-2,1),(-2,-1)]\n",
    "\n",
    "for l in logs:\n",
    "    speed = l[0]\n",
    "    torque = l[1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    rad = np.linspace(0, 5, 100)\n",
    "    azm = np.linspace(-np.pi, np.pi, 100)\n",
    "    r, th = np.meshgrid(rad, azm)\n",
    "    \n",
    "    # Q-Network\n",
    "    speed_array = speed*np.ones(r.shape)\n",
    "    torque_array = torque*np.ones(r.shape)\n",
    "    data = np.stack([np.cos(th), np.sin(th), speed_array, torque_array], axis = 2)\n",
    "    data = torch.tensor(data, dtype = torch.float32)\n",
    "    z = model_Q(data)\n",
    "    z = z.detach().numpy().squeeze()\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection=\"polar\")\n",
    "    ax1.grid(False)\n",
    "    pc = ax1.pcolormesh(th, r, z)\n",
    "    ax1.plot(azm, r, color='k', ls='none') \n",
    "    ax1.set_theta_zero_location('N')\n",
    "    fig.colorbar(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687ecb3-ad2b-4e99-90c1-24229ce802da",
   "metadata": {},
   "source": [
    "# 6) Target Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14faa5fa-d454-4e27-93b2-9c457d427c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_target(model, target_model, agent, optimizer, criterion, gamma, Buffer, epoch, device):\n",
    "    model.train()\n",
    "    b = Buffer.batch_buffer(128)\n",
    "    data = np.zeros((128,4))\n",
    "    reward = np.zeros(128)\n",
    "    next_data = np.zeros((128,4))\n",
    "    trunc = np.zeros(128)\n",
    "    for i,transition in enumerate(b):        \n",
    "        data[i] = np.concatenate([transition[0], transition[1]])\n",
    "        reward[i] = transition[2]\n",
    "        trunc[i] = transition[4]\n",
    "        next_data[i] = np.concatenate([transition[3], agent.compute_action(transition[3])])\n",
    "    \n",
    "    data = torch.tensor(data, dtype=torch.float32).to(device)   \n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "    next_data = torch.tensor(next_data, dtype=torch.float32).to(device)\n",
    "    trunc = torch.tensor(trunc, dtype = torch.bool)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)  # (state, action) -> Q\n",
    "    with torch.no_grad():\n",
    "        target = (reward + trunc*target_model(next_data).flatten()).reshape(128,1)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch_policy_target(model_policy, model_Q, target_model_policy, target_model_Q, optimizer, criterion, gamma, Buffer, epoch, device):\n",
    "    \n",
    "    model_policy.train()\n",
    "    b = Buffer.batch_buffer(128)\n",
    "    data = np.zeros((128,3))\n",
    "    reward = np.zeros(128)\n",
    "    next_data = np.zeros((128,3))\n",
    "    trunc = np.zeros(128)\n",
    "    for i,transition in enumerate(b):        \n",
    "        data[i] = transition[0]\n",
    "        reward[i] = transition[2]\n",
    "        trunc[i] = transition[4]\n",
    "        next_data[i] = transition[3]\n",
    "    \n",
    "    data = torch.tensor(data, dtype=torch.float32).to(device)   \n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "    next_data = torch.tensor(next_data, dtype=torch.float32).to(device)\n",
    "    trunc = torch.tensor(trunc, dtype = torch.bool)\n",
    "    optimizer.zero_grad()\n",
    "    output = model_policy(data)  # state -> action\n",
    "    loss = compute_loss_policy(data, output, target_model_Q)\n",
    "    # loss = criterion(loss_p, torch.zeros([128,1], requires_grad = True))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace43c79-98ed-4c24-bcb5-dd5781260681",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "# Define DDPG agent\n",
    "noise_std = 0.3\n",
    "TAU = [0.1]\n",
    "\n",
    "for tau in TAU:\n",
    "    ddpg_agent = DDPGAgent(noise_std, tau)\n",
    "\n",
    "    # \n",
    "    # model_Q = QNetwork().to(device)\n",
    "    # model_policy = PolicyNetwork().to(device)\n",
    "\n",
    "    # Training parameters, ecc...\n",
    "    lr = 1e-4\n",
    "    gamma = 0.99\n",
    "    optimizer_Q = torch.optim.Adam(ddpg_agent.Q_network.parameters(), lr = lr)\n",
    "    optimizer_policy = torch.optim.Adam(ddpg_agent.policy_network.parameters(), lr = lr)\n",
    "    criterion_Q = torch.nn.MSELoss()\n",
    "    criterion_policy = torch.nn.L1Loss()\n",
    "    num_epochs = 2000 # metti 1000\n",
    "\n",
    "    history_Q = []\n",
    "    history_policy = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        action = ddpg_agent.compute_action(state, deterministic = False) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        Buffer.store_transition((state, action, reward, next_state, trunc))\n",
    "        state = next_state\n",
    "\n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "            # Save reward of episode\n",
    "\n",
    "        loss_Q = train_epoch_target(ddpg_agent.Q_network, ddpg_agent.target_Q_network, ddpg_agent, optimizer_Q, criterion_Q, gamma, Buffer, epoch, device)\n",
    "        loss_policy = train_epoch_policy_target(ddpg_agent.policy_network, ddpg_agent.Q_network, ddpg_agent.target_policy_network, ddpg_agent.target_Q_network, optimizer_policy, criterion_policy, gamma, Buffer, epoch, device)\n",
    "\n",
    "        ddpg_agent.update_target_params\n",
    "\n",
    "        if epoch%100 == 0:\n",
    "            print(loss_Q)\n",
    "            print(loss_policy)\n",
    "        history_Q.append(loss_Q.detach().numpy())\n",
    "        history_policy.append(loss_policy.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d8e2c-271c-4d01-8ca3-383dfcfa4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(num_epochs), history_Q)\n",
    "plt.plot(np.arange(num_epochs), history_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d6c2b-096f-47c9-92d3-41cee0e80a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('Pendulum-v1', g=9.81)\n",
    "rewards = np.zeros((100,200))\n",
    "\n",
    "#Buffer = ReplayBuffer(max_size = 400)\n",
    "\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    trunc = False\n",
    "    cur_reward = []\n",
    "\n",
    "    while not trunc:\n",
    "        action = ddpg_agent.compute_action(state, deterministic = True) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        state = next_state\n",
    "        cur_reward.append(reward)\n",
    "\n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "            \n",
    "    rewards[i] = cur_reward\n",
    "\n",
    "ddpg_target_rewards = rewards\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(200), np.mean(rand_rewards,axis = 0))\n",
    "plt.plot(np.arange(200), np.mean(heur_rewards, axis = 0))\n",
    "plt.plot(np.arange(200), np.mean(ddpg_rewards, axis = 0))\n",
    "plt.plot(np.arange(200), np.mean(ddpg_target_rewards, axis = 0))\n",
    "\n",
    "\n",
    "plt.legend(['Random','Heuristic','DDPG','DDPG with targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9118fa8-a3fb-4838-8eaf-5e79bfb12782",
   "metadata": {},
   "source": [
    "# 7) Ornstein-Uhlenbeck Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae6070-8bb5-4366-b860-0d7a58a2fdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07613782-4e9f-4c61-bb25-85fb855e69bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
