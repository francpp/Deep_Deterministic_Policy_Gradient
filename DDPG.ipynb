{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac032619-0db7-45e3-941e-2847c2648aa7",
   "metadata": {},
   "source": [
    "# DDPG PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab7139-73cb-447a-a74d-a57266a2df1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import gym and define pendulum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104252cc-7c45-40c8-9ce4-0491b2261bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#env = gym.make('Pendulum-v1', g=9.81, render_mode = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07697ca8-6849-459e-baa9-2f0d30088e33",
   "metadata": {},
   "source": [
    "Check API-conformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c0451-5435-4c7d-9b30-b649bbeec6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.utils.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e5c47-7c70-43df-8674-912373b53cc8",
   "metadata": {},
   "source": [
    "Import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b7f9c-5f4e-4ac1-b67a-07c6db0e0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503181b-a59e-4c22-9e8e-37600c011295",
   "metadata": {},
   "source": [
    "## Heuristic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb838f9-6a3c-4c82-8749-f236921503b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Wrap environment in a NormalizedEnv class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966ae2b-ed43-481b-8058-bdf9ed86db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)\n",
    "#env = gym.make('Pendulum-v1', g=9.81, render_mode = 'human')\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883213a-65b7-45df-b3be-bebcb5b455eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = helpers.NormalizedEnv(env)\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28085c-6490-4b0b-82a2-5eb686020613",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f65200-e673-4d8c-8573-31fdb4b0cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = helpers.RandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0700bfc-bb4d-4106-9d64-328901544e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    \n",
    "    trunc = False\n",
    "    cur_reward = 0\n",
    "    it = 0\n",
    "    while not trunc:   \n",
    "        it+=1\n",
    "        action = random_agent.compute_action(state) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        state = next_state\n",
    "        cur_reward += reward\n",
    "        \n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "    \n",
    "    #print(it)\n",
    "    rewards.append(cur_reward)\n",
    "\n",
    "rewards = [np.sum(rewards[:i+1])/(i+1) for i in range(len(rewards))]\n",
    "print(\"Sum rewards: \",rewards)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faa4b7-9765-42b4-a235-ff9f6340461f",
   "metadata": {},
   "source": [
    "##### Heuristic pendulum agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87755dc0-fb1d-454f-aae1-d8a2df5771f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env, t):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.t = t\n",
    "    def compute_action(self, state):\n",
    "        if state[0] <= 0:\n",
    "            return self.t * np.sign(state[2])\n",
    "        else:\n",
    "            return -self.t * np.sign(state[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3a7ee-afed-40ed-9b3a-b63fc897c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_agent = HeuristicPendulumAgent(env,0.8)\n",
    "rewards = []\n",
    "\n",
    "#Buffer = ReplayBuffer(max_size = 400)\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    trunc = False\n",
    "    cur_reward = 0\n",
    "    it = 0\n",
    "    while not trunc:\n",
    "        it+= 1\n",
    "        action = heur_agent.compute_action(state) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        #Buffer.store_transition((state, action, reward, next_state, trunc))\n",
    "        state = next_state\n",
    "        cur_reward += reward\n",
    "\n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n",
    "            \n",
    "    rewards.append(cur_reward)\n",
    "    #print(it)\n",
    "\n",
    "\n",
    "#bat = Buffer.batch_buffer(3)\n",
    "#print(bat)\n",
    "rewards = [np.sum(rewards[:i+1])/(i+1) for i in range(len(rewards))]\n",
    "print(\"Sum rewards: \",rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99283af-3e3d-48e5-b053-c62d4a014882",
   "metadata": {},
   "source": [
    "Effect of fixed torque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b97b23-9993-4908-803c-ea227bd1100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torques = np.linspace(-1,1,30)\n",
    "avg_rewards = []\n",
    "\n",
    "for t in torques:\n",
    "    heur_agent = HeuristicPendulumAgent(env,t)\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "\n",
    "        trunc = False\n",
    "        cur_reward = 0\n",
    "\n",
    "        while not trunc:\n",
    "            action = heur_agent.compute_action(state) \n",
    "            (next_state, reward, term, trunc, info) = env.step(action)\n",
    "            state = next_state\n",
    "            cur_reward += reward\n",
    "\n",
    "            if term or trunc:\n",
    "                observation, info = env.reset()\n",
    "\n",
    "        rewards.append(cur_reward)\n",
    "    avg_rewards.append(rewards[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torques, avg_rewards)\n",
    "plt.xlabel('Fixed torque')\n",
    "plt.ylabel('Avg reward (over 10 runs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b527f-b284-4dd9-b4b6-639a95ea9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06883194-3076-4df2-9eed-49e43198c4a5",
   "metadata": {},
   "source": [
    "# Q function of the heuristic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edd1eb-3206-46a0-b92e-dfb913229edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def store_transition(self, trans):\n",
    "        if len(self.buffer)<self.max_size:           \n",
    "            self.buffer.append(trans)\n",
    "    \n",
    "    def batch_buffer(self, batch_size):\n",
    "        n = len(self.buffer)\n",
    "        indexes = np.random.permutation(n)[:min(batch_size,self.max_size)]\n",
    "        return [self.buffer[i] for i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059d34f-8a8a-4854-b894-f2d6382ac9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d88258-0432-4121-a535-e1d4a279291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_agent = HeuristicPendulumAgent(env,0.8)\n",
    "max_size = 1e4\n",
    "Buffer = ReplayBuffer(max_size = max_size)\n",
    "\n",
    "for _ in range(int(max_size/200)):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "\n",
    "    trunc = False\n",
    "    \n",
    "    while not trunc:\n",
    "        action = heur_agent.compute_action(state) \n",
    "        (next_state, reward, term, trunc, info) = env.step(action)\n",
    "        Buffer.store_transition((state, action, reward, next_state, trunc))\n",
    "        state = next_state\n",
    "\n",
    "        if term or trunc:\n",
    "            observation, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583916-f509-4050-bf71-d62989cd9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, agent, optimizer, criterion, gamma, Buffer, epoch, device):\n",
    "    model.train()\n",
    "    b = Buffer.batch_buffer(128)\n",
    "    data = np.zeros((128,4))\n",
    "    reward = np.zeros(128)\n",
    "    next_data = np.zeros((128,4))\n",
    "    trunc = np.zeros(128)\n",
    "    for i,transition in enumerate(b):        \n",
    "        data[i] = np.concatenate([transition[0], np.array([transition[1]])])\n",
    "        reward[i] = transition[2]\n",
    "        trunc[i] = transition[4]\n",
    "        next_data[i] = np.concatenate([transition[3], np.array([agent.compute_action(transition[3])])])\n",
    "    \n",
    "    data = torch.tensor(data, dtype=torch.float32).to(device)   \n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "    next_data = torch.tensor(next_data, dtype=torch.float32).to(device)\n",
    "    trunc = torch.tensor(trunc, dtype = torch.bool)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    with torch.no_grad():\n",
    "        target = (reward + trunc*model(next_data).flatten()).reshape(128,1)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d45f57-95c7-485a-89a1-63184cd82016",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model = QNetwork().to(device)\n",
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "history = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, heur_agent, optimizer, criterion, gamma, Buffer, epoch, device)\n",
    "    if epoch%100 == 0:\n",
    "        print(loss)\n",
    "    history.append(loss.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb09ae-d722-4e84-9a3a-223e6df959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1000), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd29bd-4ea2-4015-ac18-18c48622e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(-np.pi, np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "speed = 0\n",
    "torque = 0.8\n",
    "speed_array = speed*np.ones(r.shape)\n",
    "torque_array = torque*np.ones(r.shape)\n",
    "data = np.stack([np.cos(th), np.sin(th), speed_array, torque_array], axis = 2)\n",
    "data = torch.tensor(data, dtype = torch.float32)\n",
    "z = model(data)\n",
    "z = z.detach().numpy().squeeze()\n",
    "print(shape(z))\n",
    "plt.subplot(projection=\"polar\")\n",
    "\n",
    "plt.pcolormesh(th, r, z)\n",
    "#plt.pcolormesh(th, z, r)\n",
    "\n",
    "plt.plot(azm, r, color='k', ls='none') \n",
    "plt.grid()\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bce1f4-64cc-4c42-b7fa-678f372bf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(alpha, alpha_speed, torque):\n",
    "    return -(alpha**2 + 0.1*alpha_speed**2 + 0.001*torque**2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "rad = np.linspace(0, 5, 100)\n",
    "azm = np.linspace(-np.pi, np.pi, 100)\n",
    "r, th = np.meshgrid(rad, azm)\n",
    "speed = 0.5\n",
    "torque = 2\n",
    "plt.subplot(projection=\"polar\")\n",
    "z_heur = reward(th, speed, torque*np.sign(np.cos(th))*np.sign(speed))\n",
    "#print(z_heur)\n",
    "plt.pcolormesh(th, r, z_heur)\n",
    "#plt.pcolormesh(th, z, r)\n",
    "\n",
    "plt.plot(azm, r, color='k', ls='none') \n",
    "plt.grid()\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df36b5-94aa-43b3-b46a-e320dd959f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
